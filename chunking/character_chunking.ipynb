{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_chunking(chunk_size):\n",
    "    with open(\"../content.txt\", \"r\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(content), chunk_size):\n",
    "        chunk = content[i : i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Retrieval-Augmented Generation (RAG) is a groundbreaking technique in natural language processing that combines the strengths of retrieval-based and generative models to create a system capable of producing highly accurate and contextually relevant text. By leveraging both retrieval and generation, ',\n",
       " 'RAG models address many of the limitations of traditional models, offering a more robust and flexible approach to various tasks. The process begins with the retrieval of relevant documents or passages from a large corpus. The retrieval component typically uses dense embeddings, which are vector repr',\n",
       " 'esentations learned to capture the semantic meaning of the text. These embeddings allow the model to measure the similarity between the input query and potential documents, even when they do not share exact keywords. Dense retrieval models, often based on transformer architectures like BERT, excel a',\n",
       " 't finding contextually relevant information.\\n\\nOnce the most relevant documents are retrieved, the generative component of the RAG model takes over. Generative models, particularly those built on transformers like GPT-3 or T5, are trained to produce coherent and contextually appropriate text. In RAG,',\n",
       " ' the generative model is conditioned on the retrieved documents, which enables it to generate responses that are not only fluent but also enriched with accurate information from the retrieved content. This conditioning ensures that the responses are highly relevant to the input query, making RAG mod',\n",
       " 'els particularly effective in open-domain question-answering tasks, where the system needs to retrieve and synthesize information from various sources to provide a comprehensive answer.\\n\\nThe process of chunking, or dividing text into smaller, manageable units, plays a crucial role in the efficiency ',\n",
       " 'and accuracy of RAG models. There are several methods of chunking that can be employed depending on the nature of the text and the specific requirements of the task. One common method is sentence-based chunking, where the text is divided into individual sentences. This method is straightforward and ',\n",
       " 'ensures that each chunk is a complete, coherent thought. Another method is paragraph-based chunking, which involves dividing the text into paragraphs. This approach is useful when each paragraph contains a distinct idea or topic, allowing the model to retrieve chunks that are thematically cohesive.\\n',\n",
       " '\\nA more advanced method is character-based chunking, where the text is split based on a fixed number of characters. This method is often used when dealing with large documents or when the text needs to be divided into very fine-grained chunks. Character-based chunking can be particularly useful in c',\n",
       " 'ases where the text is highly structured or where maintaining the flow of information across chunks is critical. Additionally, sliding window chunking is another technique, where overlapping chunks are created by moving a fixed-size window across the text. This method helps to maintain context acros',\n",
       " 's chunks, as each chunk shares some content with its neighbors, ensuring that important information is not lost during the chunking process.\\n\\nThe embeddings used in RAG models are critical for the retrieval process. Dense embeddings, which are high-dimensional vector representations of text, capture',\n",
       " ' the semantic meaning of words, sentences, or documents. These embeddings are typically learned through pretraining on large corpora using models like BERT, which are designed to capture contextual relationships between words. Dense embeddings allow the retrieval model to measure the similarity betw',\n",
       " 'een the input query and potential documents in a more nuanced way than traditional keyword-based methods. This enables the model to retrieve documents that are contextually relevant, even if they do not contain the exact words used in the query.\\n\\nIn addition to dense embeddings, sparse embeddings ca',\n",
       " 'n also be used in RAG models. Sparse embeddings represent text in a high-dimensional space where most dimensions are zero. This method captures more granular, term-level information and can be useful in cases where specific words or phrases are critical to the query. Sparse embeddings are often used',\n",
       " ' in conjunction with dense embeddings in hybrid retrieval models, where the strengths of both methods are leveraged to improve retrieval accuracy.\\n\\nSimilarity calculation is another crucial aspect of RAG models. The retrieval component uses similarity metrics to determine which documents or passages',\n",
       " ' are most relevant to the input query. Common similarity metrics include cosine similarity, which measures the angle between two vectors in a high-dimensional space, and dot product similarity, which measures the magnitude of the projection of one vector onto another. These metrics are used to compa',\n",
       " 're the embeddings of the input query with the embeddings of potential documents, allowing the model to rank the documents based on their relevance. In addition to these traditional metrics, advanced techniques like contrastive learning are also used to improve similarity calculations. Contrastive le',\n",
       " \"arning involves training the model to distinguish between similar and dissimilar pairs of text, which enhances the model's ability to retrieve the most relevant information.\\n\\nOnce the retrieval component has identified the most relevant chunks, the generative component synthesizes this information t\",\n",
       " 'o produce a coherent response. This process involves integrating the content of the retrieved chunks with the input query, allowing the generative model to generate text that is both accurate and contextually appropriate. The quality of the generated text is heavily influenced by the chunking method',\n",
       " ' used, as well as the embeddings and similarity metrics employed during retrieval. Effective chunking ensures that the retrieved information is coherent and contextually relevant, while high-quality embeddings and accurate similarity calculations ensure that the most relevant information is retrieve',\n",
       " 'd.\\n\\nThe training process for RAG models involves both pretraining and fine-tuning. During pretraining, the retrieval and generative components are trained on large-scale datasets to learn dense embeddings and generate coherent text. Fine-tuning is then performed on task-specific datasets, where the ',\n",
       " 'model is trained to optimize both retrieval and generation in a joint manner. This fine-tuning process helps the model adapt to the specific requirements of the target application, improving its performance on real-world tasks.\\n\\nRAG models have shown significant promise in a variety of applications.',\n",
       " ' In knowledge-intensive tasks, RAG models can provide accurate and contextually relevant answers to complex questions by retrieving and synthesizing information from multiple sources. In summarization tasks, RAG models can generate concise and informative summaries by retrieving the most relevant pa',\n",
       " 'ssages and synthesizing them into a coherent summary. The flexibility and versatility of RAG models also make them well-suited for tasks like content generation, personalized content delivery, and conversational AI.\\n\\nIn conversational AI, RAG models can enhance the quality of interactions by retriev',\n",
       " 'ing relevant information and generating responses that are both accurate and engaging. This can be particularly useful in customer support, where the system needs to provide helpful responses to a wide range of queries. By retrieving relevant information from a vast corpus and generating contextuall',\n",
       " 'y appropriate responses, RAG models can improve the effectiveness and efficiency of conversational systems.\\n\\nRAG models also have significant potential in healthcare, where the ability to retrieve and generate accurate and contextually relevant information is crucial. In clinical decision support, R',\n",
       " 'AG models can retrieve relevant medical literature and generate evidence-based recommendations, helping healthcare professionals make informed decisions. In patient education, RAG models can generate personalized explanations of medical conditions or treatments, improving patient understanding and e',\n",
       " 'ngagement.\\n\\nThe use of RAG models in multilingual applications is another area of growing interest. By training the retrieval and generative components on multilingual datasets, RAG models can provide accurate and contextually appropriate responses in multiple languages. This capability is particula',\n",
       " 'rly valuable in applications like multilingual QA systems, where the goal is to provide accurate answers to queries in different languages. The ability to retrieve and generate content in multiple languages enhances the accessibility and usability of RAG-based systems in global contexts.\\n\\nResearch i',\n",
       " 'nto improving the efficiency and scalability of RAG models is ongoing. One area of focus is optimizing the retrieval component to ensure that the most relevant documents are retrieved quickly and accurately. Techniques such as approximate nearest neighbor search and re-ranking have been explored to ',\n",
       " 'improve retrieval efficiency. Additionally, efforts are being made to reduce the computational complexity of the generative component, enabling RAG models to generate high-quality responses in real-time.\\n\\nEthical considerations are also a crucial aspect of RAG model development and deployment. Given',\n",
       " ' their ability to retrieve and generate content, RAG models must be designed to avoid generating harmful, biased, or misleading information. Ensuring that the training data is diverse and representative of different perspectives is essential for minimizing biases in the generated content. Additional',\n",
       " 'ly, mechanisms for detecting and mitigating potential biases in both retrieval and generation are necessary for ensuring the ethical use of RAG models.\\n\\nThe interpretability of RAG models is another important consideration. As RAG models are increasingly used in critical applications, such as health',\n",
       " 'care and legal decision-making, it is important to ensure that the decisions made by these models are transparent and interpretable. Techniques for explaining the retrieval and generation processes, as well as providing insights into how the model arrives at its conclusions, are important for buildi',\n",
       " 'ng trust and accountability in RAG-based systems.\\n\\nAs research and development in RAG models continue, their potential applications are likely to expand. The ability of RAG models to retrieve and generate contextually relevant content makes them well-suited for a wide range of tasks, from informatio',\n",
       " 'n retrieval and QA to content generation and personalization. As advancements in RAG models are made, we can expect to see even more sophisticated and powerful AI systems emerge, capable of handling increasingly complex and diverse tasks.\\n\\nIn summary, Retrieval-Augmented Generation (RAG) represents ',\n",
       " 'a powerful and versatile approach to natural language processing. By combining the strengths of retrieval-based and generative models, RAG models can provide accurate, contextually relevant, and coherent responses to a wide range of queries. Their ability to handle complex, open-domain tasks, integr',\n",
       " 'ate information from multiple sources, and generate personalized content makes them a valuable tool in various applications. As research into RAG models progresses, it is likely that we will see further advancements in their capabilities, efficiency, and ethical considerations, paving the way for th',\n",
       " 'e next generation of intelligent systems.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = character_chunking(300)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings(text, model, tokeniser):\n",
    "    inputs = tokeniser(text, return_tensors = \"pt\", truncation = True, padding = True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim = 1).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(embedding1, embedding2):\n",
    "    similarity = cosine_similarity([embedding1], [embedding2])\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_model(chunks, user_query, top_k, model_name = \"bert-base-uncased\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    user_query_embedding = calculate_embeddings(user_query, model, tokenizer)\n",
    "    embeddings = [calculate_embeddings(chunk, model, tokenizer) for chunk in chunks]\n",
    "    count = [i for i in range(len(chunks))]\n",
    "    scores = {i: calculate_similarity(user_query_embedding, embedding) for embedding, i in zip(embeddings, count)}\n",
    "    sorted_chunks = sorted(scores.items(), key = lambda item: item[1])[:top_k]\n",
    "    \n",
    "    print(\"Matching chunks:\\n\")\n",
    "    for chunk in sorted_chunks:\n",
    "        print(chunks[chunk[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching chunks:\n",
      "\n",
      "a powerful and versatile approach to natural language processing. By combining the strengths of retrieval-based and generative models, RAG models can provide accurate, contextually relevant, and coherent responses to a wide range of queries. Their ability to handle complex, open-domain tasks, integr\n",
      "model is trained to optimize both retrieval and generation in a joint manner. This fine-tuning process helps the model adapt to the specific requirements of the target application, improving its performance on real-world tasks.\n",
      "\n",
      "RAG models have shown significant promise in a variety of applications.\n",
      "AG models can retrieve relevant medical literature and generate evidence-based recommendations, helping healthcare professionals make informed decisions. In patient education, RAG models can generate personalized explanations of medical conditions or treatments, improving patient understanding and e\n"
     ]
    }
   ],
   "source": [
    "test_with_model(chunks, user_query=\"What is rag?\", top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using langchain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "def langchain_character_chunking(chunk_size):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = 0)\n",
    "\n",
    "    with open(\"../content.txt\", \"r\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    chunks = text_splitter.split_text(content)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 944, which is longer than the specified 300\n",
      "Created a chunk of size 739, which is longer than the specified 300\n",
      "Created a chunk of size 712, which is longer than the specified 300\n",
      "Created a chunk of size 739, which is longer than the specified 300\n",
      "Created a chunk of size 703, which is longer than the specified 300\n",
      "Created a chunk of size 499, which is longer than the specified 300\n",
      "Created a chunk of size 925, which is longer than the specified 300\n",
      "Created a chunk of size 727, which is longer than the specified 300\n",
      "Created a chunk of size 523, which is longer than the specified 300\n",
      "Created a chunk of size 586, which is longer than the specified 300\n",
      "Created a chunk of size 490, which is longer than the specified 300\n",
      "Created a chunk of size 501, which is longer than the specified 300\n",
      "Created a chunk of size 576, which is longer than the specified 300\n",
      "Created a chunk of size 513, which is longer than the specified 300\n",
      "Created a chunk of size 546, which is longer than the specified 300\n",
      "Created a chunk of size 496, which is longer than the specified 300\n",
      "Created a chunk of size 487, which is longer than the specified 300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Retrieval-Augmented Generation (RAG) is a groundbreaking technique in natural language processing that combines the strengths of retrieval-based and generative models to create a system capable of producing highly accurate and contextually relevant text. By leveraging both retrieval and generation, RAG models address many of the limitations of traditional models, offering a more robust and flexible approach to various tasks. The process begins with the retrieval of relevant documents or passages from a large corpus. The retrieval component typically uses dense embeddings, which are vector representations learned to capture the semantic meaning of the text. These embeddings allow the model to measure the similarity between the input query and potential documents, even when they do not share exact keywords. Dense retrieval models, often based on transformer architectures like BERT, excel at finding contextually relevant information.',\n",
       " 'Once the most relevant documents are retrieved, the generative component of the RAG model takes over. Generative models, particularly those built on transformers like GPT-3 or T5, are trained to produce coherent and contextually appropriate text. In RAG, the generative model is conditioned on the retrieved documents, which enables it to generate responses that are not only fluent but also enriched with accurate information from the retrieved content. This conditioning ensures that the responses are highly relevant to the input query, making RAG models particularly effective in open-domain question-answering tasks, where the system needs to retrieve and synthesize information from various sources to provide a comprehensive answer.',\n",
       " 'The process of chunking, or dividing text into smaller, manageable units, plays a crucial role in the efficiency and accuracy of RAG models. There are several methods of chunking that can be employed depending on the nature of the text and the specific requirements of the task. One common method is sentence-based chunking, where the text is divided into individual sentences. This method is straightforward and ensures that each chunk is a complete, coherent thought. Another method is paragraph-based chunking, which involves dividing the text into paragraphs. This approach is useful when each paragraph contains a distinct idea or topic, allowing the model to retrieve chunks that are thematically cohesive.',\n",
       " 'A more advanced method is character-based chunking, where the text is split based on a fixed number of characters. This method is often used when dealing with large documents or when the text needs to be divided into very fine-grained chunks. Character-based chunking can be particularly useful in cases where the text is highly structured or where maintaining the flow of information across chunks is critical. Additionally, sliding window chunking is another technique, where overlapping chunks are created by moving a fixed-size window across the text. This method helps to maintain context across chunks, as each chunk shares some content with its neighbors, ensuring that important information is not lost during the chunking process.',\n",
       " 'The embeddings used in RAG models are critical for the retrieval process. Dense embeddings, which are high-dimensional vector representations of text, capture the semantic meaning of words, sentences, or documents. These embeddings are typically learned through pretraining on large corpora using models like BERT, which are designed to capture contextual relationships between words. Dense embeddings allow the retrieval model to measure the similarity between the input query and potential documents in a more nuanced way than traditional keyword-based methods. This enables the model to retrieve documents that are contextually relevant, even if they do not contain the exact words used in the query.',\n",
       " 'In addition to dense embeddings, sparse embeddings can also be used in RAG models. Sparse embeddings represent text in a high-dimensional space where most dimensions are zero. This method captures more granular, term-level information and can be useful in cases where specific words or phrases are critical to the query. Sparse embeddings are often used in conjunction with dense embeddings in hybrid retrieval models, where the strengths of both methods are leveraged to improve retrieval accuracy.',\n",
       " \"Similarity calculation is another crucial aspect of RAG models. The retrieval component uses similarity metrics to determine which documents or passages are most relevant to the input query. Common similarity metrics include cosine similarity, which measures the angle between two vectors in a high-dimensional space, and dot product similarity, which measures the magnitude of the projection of one vector onto another. These metrics are used to compare the embeddings of the input query with the embeddings of potential documents, allowing the model to rank the documents based on their relevance. In addition to these traditional metrics, advanced techniques like contrastive learning are also used to improve similarity calculations. Contrastive learning involves training the model to distinguish between similar and dissimilar pairs of text, which enhances the model's ability to retrieve the most relevant information.\",\n",
       " 'Once the retrieval component has identified the most relevant chunks, the generative component synthesizes this information to produce a coherent response. This process involves integrating the content of the retrieved chunks with the input query, allowing the generative model to generate text that is both accurate and contextually appropriate. The quality of the generated text is heavily influenced by the chunking method used, as well as the embeddings and similarity metrics employed during retrieval. Effective chunking ensures that the retrieved information is coherent and contextually relevant, while high-quality embeddings and accurate similarity calculations ensure that the most relevant information is retrieved.',\n",
       " 'The training process for RAG models involves both pretraining and fine-tuning. During pretraining, the retrieval and generative components are trained on large-scale datasets to learn dense embeddings and generate coherent text. Fine-tuning is then performed on task-specific datasets, where the model is trained to optimize both retrieval and generation in a joint manner. This fine-tuning process helps the model adapt to the specific requirements of the target application, improving its performance on real-world tasks.',\n",
       " 'RAG models have shown significant promise in a variety of applications. In knowledge-intensive tasks, RAG models can provide accurate and contextually relevant answers to complex questions by retrieving and synthesizing information from multiple sources. In summarization tasks, RAG models can generate concise and informative summaries by retrieving the most relevant passages and synthesizing them into a coherent summary. The flexibility and versatility of RAG models also make them well-suited for tasks like content generation, personalized content delivery, and conversational AI.',\n",
       " 'In conversational AI, RAG models can enhance the quality of interactions by retrieving relevant information and generating responses that are both accurate and engaging. This can be particularly useful in customer support, where the system needs to provide helpful responses to a wide range of queries. By retrieving relevant information from a vast corpus and generating contextually appropriate responses, RAG models can improve the effectiveness and efficiency of conversational systems.',\n",
       " 'RAG models also have significant potential in healthcare, where the ability to retrieve and generate accurate and contextually relevant information is crucial. In clinical decision support, RAG models can retrieve relevant medical literature and generate evidence-based recommendations, helping healthcare professionals make informed decisions. In patient education, RAG models can generate personalized explanations of medical conditions or treatments, improving patient understanding and engagement.',\n",
       " 'The use of RAG models in multilingual applications is another area of growing interest. By training the retrieval and generative components on multilingual datasets, RAG models can provide accurate and contextually appropriate responses in multiple languages. This capability is particularly valuable in applications like multilingual QA systems, where the goal is to provide accurate answers to queries in different languages. The ability to retrieve and generate content in multiple languages enhances the accessibility and usability of RAG-based systems in global contexts.',\n",
       " 'Research into improving the efficiency and scalability of RAG models is ongoing. One area of focus is optimizing the retrieval component to ensure that the most relevant documents are retrieved quickly and accurately. Techniques such as approximate nearest neighbor search and re-ranking have been explored to improve retrieval efficiency. Additionally, efforts are being made to reduce the computational complexity of the generative component, enabling RAG models to generate high-quality responses in real-time.',\n",
       " 'Ethical considerations are also a crucial aspect of RAG model development and deployment. Given their ability to retrieve and generate content, RAG models must be designed to avoid generating harmful, biased, or misleading information. Ensuring that the training data is diverse and representative of different perspectives is essential for minimizing biases in the generated content. Additionally, mechanisms for detecting and mitigating potential biases in both retrieval and generation are necessary for ensuring the ethical use of RAG models.',\n",
       " 'The interpretability of RAG models is another important consideration. As RAG models are increasingly used in critical applications, such as healthcare and legal decision-making, it is important to ensure that the decisions made by these models are transparent and interpretable. Techniques for explaining the retrieval and generation processes, as well as providing insights into how the model arrives at its conclusions, are important for building trust and accountability in RAG-based systems.',\n",
       " 'As research and development in RAG models continue, their potential applications are likely to expand. The ability of RAG models to retrieve and generate contextually relevant content makes them well-suited for a wide range of tasks, from information retrieval and QA to content generation and personalization. As advancements in RAG models are made, we can expect to see even more sophisticated and powerful AI systems emerge, capable of handling increasingly complex and diverse tasks.',\n",
       " 'In summary, Retrieval-Augmented Generation (RAG) represents a powerful and versatile approach to natural language processing. By combining the strengths of retrieval-based and generative models, RAG models can provide accurate, contextually relevant, and coherent responses to a wide range of queries. Their ability to handle complex, open-domain tasks, integrate information from multiple sources, and generate personalized content makes them a valuable tool in various applications. As research into RAG models progresses, it is likely that we will see further advancements in their capabilities, efficiency, and ethical considerations, paving the way for the next generation of intelligent systems.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = langchain_character_chunking(300)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching chunks:\n",
      "\n",
      "Once the retrieval component has identified the most relevant chunks, the generative component synthesizes this information to produce a coherent response. This process involves integrating the content of the retrieved chunks with the input query, allowing the generative model to generate text that is both accurate and contextually appropriate. The quality of the generated text is heavily influenced by the chunking method used, as well as the embeddings and similarity metrics employed during retrieval. Effective chunking ensures that the retrieved information is coherent and contextually relevant, while high-quality embeddings and accurate similarity calculations ensure that the most relevant information is retrieved.\n",
      "RAG models have shown significant promise in a variety of applications. In knowledge-intensive tasks, RAG models can provide accurate and contextually relevant answers to complex questions by retrieving and synthesizing information from multiple sources. In summarization tasks, RAG models can generate concise and informative summaries by retrieving the most relevant passages and synthesizing them into a coherent summary. The flexibility and versatility of RAG models also make them well-suited for tasks like content generation, personalized content delivery, and conversational AI.\n",
      "In summary, Retrieval-Augmented Generation (RAG) represents a powerful and versatile approach to natural language processing. By combining the strengths of retrieval-based and generative models, RAG models can provide accurate, contextually relevant, and coherent responses to a wide range of queries. Their ability to handle complex, open-domain tasks, integrate information from multiple sources, and generate personalized content makes them a valuable tool in various applications. As research into RAG models progresses, it is likely that we will see further advancements in their capabilities, efficiency, and ethical considerations, paving the way for the next generation of intelligent systems.\n"
     ]
    }
   ],
   "source": [
    "test_with_model(chunks, user_query=\"What is rag?\", top_k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
